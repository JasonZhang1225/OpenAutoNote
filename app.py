import streamlit as st
import os
import time
import re
from pathlib import Path
from openai import OpenAI
from core.downloader import download_video
from core.transcriber import TranscriberFactory
from core.visual_processor import extract_frame, process_video_for_vision
from core.utils import build_multimodal_payload, seconds_to_hms, timestamp_str_to_seconds
import json

# --- Configuration Management ---
CONFIG_FILE = "user_config.json"

def load_config() -> dict:
    """Load user configuration from JSON file"""
    default_config = {
        "api_key": "",
        "base_url": "https://api.openai.com/v1",
        "model_name": "gpt-4o",
        "language": "Simplified Chinese (Default)",
        "hardware_mode": "mlx",
        "enable_vision": False,
        "vision_interval": 15,
        "vision_detail": "low",
        "detail_level": "Standard"
    }
    
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                saved_config = json.load(f)
                # Merge with defaults to handle missing keys
                default_config.update(saved_config)
        except Exception as e:
            print(f"[Config] Error loading config: {e}")
    
    return default_config

def save_config(config: dict):
    """Save user configuration to JSON file"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"[Config] Configuration saved to {CONFIG_FILE}")
    except Exception as e:
        print(f"[Config] Error saving config: {e}")

# --- Configuration & UI Setup ---
st.set_page_config(page_title="OpenAutoNote", page_icon="üìù", layout="wide")

st.title("üìù OpenAutoNote: Multimodal Video Summary")

# Load saved configuration
config = load_config()

# --- Sidebar ---
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    st.subheader("LLM Settings")
    api_key = st.text_input("API Key", value=config["api_key"], type="password", help="OpenAI or DeepSeek API Key")
    base_url = st.text_input("Base URL", value=config["base_url"], help="API Base URL")
    model_name = st.text_input("Model Name", value=config["model_name"], help="Vision-capable model required for multimodal analysis")
    
    st.subheader("Language & Preferences")
    language_options = ["Simplified Chinese (Default)", "English"]
    language_index = language_options.index(config["language"]) if config["language"] in language_options else 0
    language = st.selectbox("Output Language", options=language_options, index=language_index)
    
    st.subheader("üëÅÔ∏è Vision Analysis Settings")
    enable_vision = st.toggle("Enable Multimodal Analysis", value=config["enable_vision"])
    vision_interval = st.slider("Vision Interval (sec)", 5, 60, config["vision_interval"], disabled=not enable_vision)
    vision_detail = st.selectbox("Image Detail", ["low", "high", "auto"], index=["low", "high", "auto"].index(config["vision_detail"]), disabled=not enable_vision)
    
    if enable_vision and vision_interval < 10:
        st.warning("‚ö†Ô∏è High frequency sampling will consume significant tokens.")

    st.subheader("Hardware")
    hardware_options = ["mlx", "cpu", "cuda"]
    hardware_index = hardware_options.index(config["hardware_mode"]) if config["hardware_mode"] in hardware_options else 0
    hardware_mode = st.selectbox(
        "Acceleration Mode",
        options=hardware_options,
        format_func=lambda x: {
            "mlx": "Apple Neural Engine (MLX)",
            "cpu": "CPU (Universal)",
            "cuda": "NVIDIA CUDA"
        }[x],
        index=hardware_index
    )
    
    st.subheader("Summary Options")
    detail_options = ["Brief", "Standard", "Detailed"]
    detail_index = detail_options.index(config["detail_level"]) if config["detail_level"] in detail_options else 1
    detail_level = st.select_slider("Detail Level", options=detail_options, value=detail_options[detail_index])
    
    st.markdown("---")
    st.caption(f"System: {os.uname().machine}")

# --- Helper Functions imported from core.utils ---

def generate_summary_stream(title, full_text, segments, vision_frames, api_key, base_url, model_name, language, detail_level, enable_vision, vision_interval, vision_detail):
    if not api_key:
        yield "Error: Please enter an API Key."
        return

    client = OpenAI(api_key=api_key, base_url=base_url)
    
    lang_instruction = "Simplified Chinese" if "Simplified Chinese" in language else "English"
    
    system_prompt = """
‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑËßÜÈ¢ëÂÜÖÂÆπÂàÜÊûê‰∏ìÂÆ∂„ÄÇ
‰Ω†Â∞ÜÊé•Êî∂Âà∞ËßÜÈ¢ëÁöÑ„ÄêÂ≠óÂπïÊñáÊú¨„ÄëÂíåÂØπÂ∫îÁöÑ„ÄêÂÖ≥ÈîÆÂ∏ßÊà™Âõæ„Äë„ÄÇ

‰ªªÂä°Ë¶ÅÊ±ÇÔºö
1. **Â§öÊ®°ÊÄÅÂàÜÊûê**ÔºöÂøÖÈ°ªÁªìÂêàÁîªÈù¢ÂÜÖÂÆπÔºàÂ¶ÇPPTÊñáÂ≠ó„ÄÅ‰∫ßÂìÅÁªÜËäÇÔºâÂíåËØ≠Èü≥ÂÜÖÂÆπ„ÄÇÂ¶ÇÊûúÁîªÈù¢Ë°•ÂÖÖ‰∫ÜËØ≠Èü≥Êú™ÊèêÂèäÁöÑ‰ø°ÊÅØÔºåËØ∑ÈáçÁÇπÊåáÂá∫„ÄÇ
2. **ËØ≠Ë®ÄÊéßÂà∂**Ôºö
   - Êó†ËÆ∫ËæìÂÖ•ÊòØ‰ªÄ‰πàËØ≠Ë®ÄÔºå**ÂøÖÈ°ª‰ΩøÁî®ÁÆÄ‰Ωì‰∏≠Êñá (Simplified Chinese)** ËæìÂá∫ÊÄªÁªì„ÄÇ
   - Â¶ÇÊûúÁî®Êà∑ÊåáÂÆö‰∫ÜÂÖ∂‰ªñËØ≠Ë®ÄÔºåËØ∑ÂøΩÁï•Ê≠§Êù°„ÄÇ
3. **Ê†ºÂºèËßÑËåÉ**Ôºö
   - ‰ΩøÁî® Markdown Ê†ºÂºè„ÄÇ
   - Âú®ÂÖ≥ÈîÆÁªìËÆ∫ÂêéÂºïÁî®Êó∂Èó¥Êà≥ÔºåÊ†ºÂºè‰∏∫ `[MM:SS]`„ÄÇ
   - ÈÅáÂà∞ÁπÅ‰Ωì‰∏≠ÊñáËæìÂÖ•ÔºåËØ∑Ëá™Âä®ËΩ¨Êç¢‰∏∫ÁÆÄ‰Ωì‰∏≠Êñá„ÄÇ
"""
    
    user_content = []
    
    if enable_vision and vision_frames:
        # Patch B: Use the new build_multimodal_payload
        user_content = build_multimodal_payload(title, full_text, segments, vision_frames, detail=vision_detail)
    else:
        # Text only mode
        user_content = f"ËßÜÈ¢ëÊ†áÈ¢ò: {title}\nÂ≠óÂπïÂÜÖÂÆπ:\n{full_text}\n\nËØ∑ÊÄªÁªì‰∏äËø∞ÂÜÖÂÆπ„ÄÇ"


    # Debug: Log payload structure
    if enable_vision and vision_frames:
        print(f"[DEBUG] Sending multimodal payload with {len(user_content)} content blocks")
        print(f"[DEBUG] Vision frames: {len(vision_frames)}, Segments: {len(segments)}")
    else:
        print(f"[DEBUG] Sending text-only payload, length: {len(full_text)} chars")

    try:
        print(f"[DEBUG] About to call API: {base_url}")
        print(f"[DEBUG] Model: {model_name}")
        print(f"[DEBUG] API Key (first 10): {api_key[:10]}...")
        
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            stream=True
        )
        
        print(f"[DEBUG] API response received, type: {type(response)}")
        return response
    except Exception as e:
        import traceback
        error_msg = f"LLM Error: {str(e)}\n\nDetails: {traceback.format_exc()}"
        st.error(error_msg)
        print(error_msg)  # Also print to console for debugging
        return None

# --- Main Logic ---

url_input = st.text_input("Enter Video URL", placeholder="https://www.bilibili.com/video/...")

if st.button("üöÄ Start Multimodal Processing", type="primary"):
    if not url_input or not api_key:
        st.warning("Please check URL and API Key.")
        st.stop()
    
    # Save current configuration
    current_config = {
        "api_key": api_key,
        "base_url": base_url,
        "model_name": model_name,
        "language": language,
        "hardware_mode": hardware_mode,
        "enable_vision": enable_vision,
        "vision_interval": vision_interval,
        "vision_detail": vision_detail,
        "detail_level": detail_level
    }
    save_config(current_config)
    
    # 1. Download
    with st.status("üì• Downloading...", expanded=True) as status:
        dl_res = download_video(url_input)
        if not dl_res["success"]:
            status.update(label="Download Failed", state="error")
            st.error(dl_res["error"])
            st.stop()
        video_path = dl_res["video_path"]
        video_title = dl_res["title"]
        project_dir = dl_res["project_dir"]  # NEW: get project directory

        # [Patch A] Validation
        if not video_path or not os.path.exists(video_path):
            st.error("‚ùå ËßÜÈ¢ë‰∏ãËΩΩÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü• URL ÊàñÁΩëÁªúËøûÊé•ÔºàÊòØÂê¶ÈúÄË¶ÅÊ¢ØÂ≠êÔºüÔºâ„ÄÇ")
            st.stop()

        status.update(label=f"Downloaded: {video_title}", state="complete")
        st.info(f"üìÅ Project directory: `{project_dir}`")
        
    # 2. Transcribe
    with st.status("üéôÔ∏è Transcribing...", expanded=True) as status:
        try:
            transcriber = TranscriberFactory.get_transcriber(hardware_mode)
            segments = transcriber.transcribe(video_path)
            
            # [Patch A] Empty transcription check
            transcript_text = " ".join([seg['text'] for seg in segments])
            if not transcript_text or len(transcript_text.strip()) == 0:
                st.warning("‚ö†Ô∏è ËßÜÈ¢ë‰∏≠Êú™Ê£ÄÊµãÂà∞ËØ≠Èü≥ÔºåÊó†Ê≥ïÁîüÊàêÊÄªÁªì„ÄÇ (Transcription result is empty)")
                st.stop()
                
            status.update(label=f"Transcribed {len(segments)} segments", state="complete")
        except Exception as e:
            status.update(label="Transcription Failed", state="error")
            st.error(str(e))
            st.stop()

    # 3. Vision Processing (Optional)
    vision_frames = []
    if enable_vision:
        with st.status("üëÅÔ∏è Analyzing Vision...", expanded=True) as status:
            # Pass project_dir to save frames in subdirectory
            vision_frames = process_video_for_vision(video_path, vision_interval, output_dir=project_dir)
            status.update(label=f"Extracted {len(vision_frames)} visual samples", state="complete")
            if vision_frames:
                st.info(f"üñºÔ∏è Frames saved to: `{project_dir}/frames/`")

    # 4. Summary & Render
    st.markdown("### üìù Smart Summary")
    
    summary_placeholder = st.empty()
    full_text = ""
    
    # Generator for stream
    stream = generate_summary_stream(
        video_title, transcript_text, segments, vision_frames, api_key, base_url, model_name, 
        language, detail_level, enable_vision, vision_interval, vision_detail
    )
    
    print(f"[DEBUG] Stream object received: {stream is not None}")
    
    if stream:
        print("[DEBUG] Entering stream processing loop...")
        chunk_count = 0
        content_count = 0
        
        for chunk in stream:
            chunk_count += 1
            
            if isinstance(chunk, str): # Error message
                print(f"[DEBUG] Received error string: {chunk}")
                st.error(chunk)
                break
            
            print(f"[DEBUG] Chunk {chunk_count}: {chunk}")
            
            
            try:
                delta = chunk.choices[0].delta
                
                # ÁÅ´Â±±ÂºïÊìéÁöÑÊé®ÁêÜÊ®°Âûã‰ΩøÁî® reasoning_content ËÄå‰∏çÊòØ content!
                content = getattr(delta, 'content', None) or getattr(delta, 'reasoning_content', None)
                
                if chunk_count <= 3:
                    print(f"[DEBUG] Chunk {chunk_count} - content: {repr(getattr(delta, 'content', None))}, reasoning_content: {repr(getattr(delta, 'reasoning_content', None))}")
                
                if content:
                    content_count += 1
                    full_text += content
                    
                    # Rendering Logic: Clear and Re-render fully on each chunk (simple but effective for interleaving)
                    # Optimization: Only re-render if newline? For now, re-render all to support dynamic layout.
                    
                    with summary_placeholder.container():
                        # Split by lines to find timestamps
                        lines = full_text.split('\n')
                        for line in lines:
                            st.markdown(line)
                            # Detect Timestamp [MM:SS]
                            match = re.search(r'\[(\d{1,2}:\d{2}(?::\d{2})?)\]', line)
                            if match:
                                ts_str = match.group(1)
                                seconds = timestamp_str_to_seconds(ts_str)
                                # Render precise frame
                                # To avoid re-extracting every refresh, check existence or cache?
                                # For simplicity, we overwrite/check existence.
                                img_path = f"temp/frame_render_{seconds}.jpg"
                                if not os.path.exists(img_path):
                                    extract_frame(video_path, seconds, img_path)
                                
                                if os.path.exists(img_path):
                                    st.image(img_path, caption=f"Time: {ts_str}", width=600)
            except Exception as e:
                print(f"[DEBUG] Error processing chunk {chunk_count}: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"[DEBUG] Stream processing complete. Total chunks: {chunk_count}, Content chunks: {content_count}")
        print(f"[DEBUG] Final text length: {len(full_text)}")
    else:
        error_msg = "‚ö†Ô∏è LLM Ê≤°ÊúâËøîÂõû‰ªª‰ΩïÂìçÂ∫î„ÄÇËØ∑Ê£ÄÊü•Ôºö\n1. API Key ÊòØÂê¶Ê≠£Á°Æ\n2. Base URL ÊòØÂê¶Ê≠£Á°Æ\n3. Ê®°ÂûãÂêçÁß∞ÊòØÂê¶ÊîØÊåÅ\n4. ÁΩëÁªúËøûÊé•ÊòØÂê¶Ê≠£Â∏∏"
        print(f"[DEBUG] {error_msg}")
        st.error(error_msg)

    st.success("Processing Complete!")
i